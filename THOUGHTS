== Random stuff ==

* POSIX: JNI, JNA, external process?
** JNI/JNA is annoying for build reasons. External process for communication reasons, but siomple to provide a portable .c file + Makefile.
** Would always be possible to use shastity without native integration for doing things like just backing up a hierarchy of images where you don't care.
* Release: Make official .jar releases with no dependency other than JDK (and C compiler for native parts if needed).
** Users to be encouraged to store such things safely with their backups or elsewhere to not depend on github/upstream.

== Block PUT scheduling ==

If there are many small blocks and many big blocks in the queue, they should
be scheduled so that big ones run in the background while the small ones
pop in and out. This is because big blocks are limited by bandwidth and small
ones by latency.

A fairly simple algorithm here might be to have two logical queues;
one for small items and one for big ones.

It would have the nice benefit that the concurrency can be kept high
for latency critical aspects that are cheap (in terms of memory) to
keep highly concurrent, while limiting extreme concurrencies for big
data that might cause memory to become a bottleneck in trying to
achieve wire speeds.

A problem is that it means that once one queue is full, a persist
process must still continue and not block on the queue. So it will
have to queue up stuff to be done later in a safe way that doesn't
cause memory consumption issues.

I think a good first solution is to not introduce complexity at all,
and just have a concurrency which is defined by memory use and have
that limit be at least N times the block size, N being the concurrency
needed to saturate whatever you want to saturate when transfering
large files. This will mean that concurrency is actually continuously
dynamic depending on the size of things being uploaded - though of
course subject to some maximum hard concurrency limit which may be
relevant depending on backend.

== GC ==
In order to GC you need access to all crypto keys, or you won't be able to
read the manifests to know what blocks are in use, and you won't know the
encrypted names of the blocks when you want to delete them.

== Crypto ==
Use GPG for data encryption? KeyCzar?
Manifest could contain which crypto wrapper to use for a given block.
That way you don't have to re-upload if you change your mind.

UPDATE: KeyCzar doesn't seem very alive, people have issues with it,
and format isn't guaranteed. GPG is out because of shell tool reasons
among other things. Let's just go for using standard Java crypto and
doing our own, at the cost of having to get some input from crypto
people to confirm with reasonable certainty that we're doing things
correctly from a security standpoint.

Should provide an easy way (cmdline options) for someone to decrypt
and encrypt individual blocks independently of the persist/depersist
process. Should also be exposed as a library so that a developer can
easily write a tool that does encryption/decryption without
re-factoring the shastity code base...

== Amazon MD5 ==
Store MD5 as amazon metadata.
Also double-check when materializing.

UPDATE: Why? (Split people editing this file.) We have our own
checksum anyway. But yes, MD5 should certainly be set during the
upload process by the S3 backend. But I don't see a need to consider
backend specific checksumming outside of the backend.

== Manifest format ==
Primary goals are:

* (1) Very easy to manually inspect/use/understand and deal with via shell tools
* (2) Must scale to large backups (hundreds of millions of files)
* (3) Must scale to large individual files (at least 1 TB)

We will propose a file format and then look at how the requirements
are fulfilled.

Let us apply compression (e.g. gzip, or bzip2, or blocked gzip) to the
entire file so we do not need to be very careful about being verbose
as long as it's repeated and compressible.

Let's have the manifest be line-oriented, and start with a single
line:

  shastity 1.0 manifest # this is a backup manifest for shastity

And then a list of lines, sorted on path, of:

  $path $meta $hash1 $hash2 ... $hashN

Assume that path and meta are both encoded with a simple escaping
mechanism to eliminate any whitespace and generally non-ascii stuff
for easy handling in a shell.

Further:

* path is the plain path
* meta is a not-so-human-readable data structure (probably a clojure map) of
  meta-data; here we can stuff arbitrary stuff in an extensible manner without
  breaking basic simplicity. having "arbitrary blob" means that from a development
  standpoint we don't need a lot of code to handle this, and can just serialize a clojure
  data structure - done. it's still not super-opaque though if someone is truly in a pinch.
* hashes are hexdigests
* there may be zero hashes (e.g., in the case of a symlink)

How does this stack up?

(1) is very well catered do. You can obtain a list of blocks to append
to construct a file by doing stuff like:

  `cat manifest | egrep ^/path/to/file\ | tr \ \\n | tail -n +3`

And restoring it is as easy as:

  `for block in $(cat manifest | egrep ^/path/to/file\ | tr \ \\n | tail -n +3) ; do store-get $block >> file ; done`

Assuming `store-get` is something which grabs a block from
store. Obviously slight adjustments may be necessary.

(2) mainly has to do with the size and handling of a manifest. A very
non-scientific test is a "find ." in my home directory containing a
mix of manually cared-for data files and stuff like vcs checkouts of
non-trivial projects like openjdk. I had 347 thousand files, and the
file listing became 28 MB in size. After gzip compression it's down to
2.1 MB. Let's say that a 10x compression ratio is pretty okay. Let's
further arbitrarily use 100 million files as a test case. Let's assume
the typical path is roughly 50 bytes long, and is compressible by 10x
by gzip.

With those numbers, 100 million paths translate into 476 MBs of
manifest data for the paths. Even assuming a total data size of 1k per
file including fs overhead, that's pretty reasonable (< 1% of data is
manifest data).

hexdigests are another matter, but can be efficiently tweaked
depending on what's being backed up and there is an obvious
relationship between block size and manifest size. If you truly have
huge files with a huge number of changes spread out and want a small
block size - then yeah, you're going to have to have a lot of meta
data. Tough. :)

So (3), already touched upon: We're fine. Let's assume a 1 TB file and
a 5 MB block size. That's roughly 200k of memory per hexdigest byte or
8 MB of text assuming 40 byte hex digests. This is *quite* reasonable
to have as a single line of text; shells won't explode, memory use
won't explode when doing a read-line operation in code, etc.

So, with the only streaming need being to stream the entire file, we
can treat each line in the file as an object and process it with
reasonable memory use. We are using a reasonable amount of disk space
and bandwidth for the manifests in relation to the amount of data and
the amount of files being backed up.

We have no O(n) memory use for any N which is not "reasonably
bounded".

For incremental backups, nothing prevents from using the manifest
format incrementally to some extent (though it adds complexity and
makes manifests non-standalone). Typical use-case should probably be
non-incremental, but incremental would be nice when e.g. doing
continuous backups based on file system change monitoring and things
like that.

